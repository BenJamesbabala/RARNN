{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "USE_CUDA = False\n",
    "SHOW_ATTENTION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nalgene.generate import *\n",
    "\n",
    "parsed = parse_file('.', 'grammar.nlg')\n",
    "parsed.map_leaves(tokenizeLeaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from ../pytorch-seq2seq-intent-parsing/data/glove.twitter.27B.100d.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import load_word_vectors\n",
    "\n",
    "def tokenize_sentence(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'(\\d)', r'\\1 ', s)\n",
    "    s = re.sub(r'[^a-z0-9 \\']', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s.split(' ')\n",
    "\n",
    "class GloVeLang:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        base_dir = '../pytorch-seq2seq-intent-parsing/data/'\n",
    "        glove_dict, glove_arr, glove_size = load_word_vectors(base_dir, 'glove.twitter.27B', size)\n",
    "        self.glove_dict = glove_dict\n",
    "        self.glove_arr = glove_arr\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s(size = %d)\" % (self.__class__.__name__, self.size)\n",
    "\n",
    "    def vector_from_word(self, word):\n",
    "        if word in self.glove_dict:\n",
    "            return self.glove_arr[self.glove_dict[word]]\n",
    "        else:\n",
    "            return torch.zeros(self.size)\n",
    "\n",
    "    def tokens_to_tensor(self, words):\n",
    "        tensor = torch.zeros(len(words), 1, self.size)\n",
    "        for wi in range(len(words)):\n",
    "            word = words[wi]\n",
    "            tensor[wi][0] = self.vector_from_word(word)\n",
    "        return tensor\n",
    "\n",
    "input_lang = GloVeLang(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descend(node, fn, child_type='phrase', returns=None):\n",
    "    if returns is None: returns = []\n",
    "    returned = fn(node)\n",
    "    returns.append(returned)\n",
    "\n",
    "    for child in node.children:\n",
    "        if (child_type is None) or (child.type == child_type):\n",
    "            descend(child, fn, child_type, returns)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ascend(node, fn):\n",
    "    if node.parent is None:\n",
    "        return fn(node)\n",
    "    else:\n",
    "        return ascend(node.parent, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building input and output vocabularies\n",
    "\n",
    "To find all input vocabulary tokens, we can traverse the parsed nalgene tree and copy all `word` type tokens.\n",
    "\n",
    "**TODO**: Use GloVe vectors for input vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input_tokens = []\n",
    "\n",
    "# def get_input_tokens(node):\n",
    "#     if node.type == 'word':\n",
    "#         input_tokens.append(node.key)\n",
    "\n",
    "# descend(parsed, get_input_tokens, None)\n",
    "\n",
    "# input_tokens = list(set(input_tokens))\n",
    "# input_tokens = ['EOS'] + input_tokens\n",
    "# print(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For output tokens, we can just take the top level node names that are either phrases or variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EOS', '%', '%main', '%if', '%timer', '%alarm', '%reminder', '%sequence', '%time', '%relative_time', '%absolute_time', '%condition', '%compare', '%getValue', '%action', '%alert', '%message', '%message_type', '@sms', '@email', '@call', '@chat', '%getLightState', '%getSwitchState', '%getTemperature', '%getPrice', '%setLightState', '%setSwitchState', '%setTemperature', '%setVolume', '%playMusic', '%greeting', '%thanks', '$message', '%operator', '@equal', '@greater_than', '@less_than', '%asset', '@btc', '@eth', '$stock', '%room_name', '@office', '@living_room', '@bedroom', '@basement', '@kitchen', '@bathroom', '@outside', '%light_name', '@office_light', '@living_room_light', '@bedroom_light', '@basement_light', '@kitchen_light', '@bathroom_light', '@outside_light', '%switch_name', '@tea_switch', '@coffee_switch', '%light_state', '%switch_state', '%volume_state', '@on', '@off', '@up', '@down', '@high', '@low', '$color', '$number', '$digits', '$digit', '%time_unit', '@seconds', '@minutes', '@hours', '@days', '$temperature', '$time', '$hour', '$minute', '$artist_name', '$song_name']\n"
     ]
    }
   ],
   "source": [
    "output_tokens = [child.key for child in parsed.children if child.type in ['phrase', 'ref', 'value']]\n",
    "output_tokens = ['EOS'] + output_tokens\n",
    "print(output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting input and target data for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_for_position(words, position):\n",
    "    if position is None:\n",
    "        return words\n",
    "    start, end, length = position\n",
    "    return words[start : end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relative_position(node, parent):\n",
    "    if parent.position is None:\n",
    "        return node.position\n",
    "    return node.position[0] - parent.position[0], node.position[1] - parent.position[0], node.position[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_for_node(flat, node):\n",
    "    words = [child.key for child in flat.children]\n",
    "    inputs = words_for_position(words, node.position)\n",
    "    keys = [child.key for child in node.children]\n",
    "    positions = [relative_position(child, node) for child in node.children]\n",
    "    return node.key, inputs, list(zip(keys, positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensors for input and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_tensor(tokens, source_tokens, append_eos=True):\n",
    "    indexes = []\n",
    "    for token in tokens:\n",
    "        indexes.append(source_tokens.index(token))\n",
    "    if append_eos:\n",
    "        indexes.append(0)\n",
    "    return torch.LongTensor(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ranges_to_tensor(ranges, seq_len):\n",
    "    ranges_tensor = torch.zeros(len(ranges), seq_len)\n",
    "    for r in range(len(ranges)):\n",
    "        start, end, _ = ranges[r]\n",
    "        ranges_tensor[r, start:end+1] = 1\n",
    "    return ranges_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The core model is a regular seq2seq/encoder-decoder model with attention. The attention model is from [Luong et al.'s \"Effective Approaches to Attention-based Neural Machine Translation\"](https://arxiv.org/abs/1508.04025) using dot-product based attention energies, with one important difference: there is no softmax layer, allowing attention to focus on multiple tokens at once. Instead a sigmoid layer is added to squeeze outputs between 0 and 1.\n",
    "\n",
    "The encoder and decoder take one additional input `context` which represents the type of phrase, e.g. `%setLightState`. At the top level node the context is always `%`.\n",
    "\n",
    "The encoder encodes the input sequence into a series of vectors using a bidirectional GRU. The decoder \"translates\" this into a sequence of phrase tokens, given the encoder outputs and current context, e.g. \"turn off the office light\" + `%setLightState` &rarr; `[$on_off, $light]`.\n",
    "\n",
    "Once the decoder has chosen tokens and alignments, the phrase tokens and selection of inputs are used as the context and inputs of the next iteration. This recurs until no more phrase tokens are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.05):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, context_input, word_inputs):\n",
    "        # TODO: Incorporate context input\n",
    "        # TODO: Batching\n",
    "        \n",
    "        seq_len = word_inputs.size(0)\n",
    "        batch_size = word_inputs.size(1)\n",
    "        \n",
    "        embedded = self.embedding(word_inputs.view(seq_len * batch_size, -1)) # Process seq x batch at once\n",
    "        output = embedded.view(seq_len, batch_size, -1) # Resize back to seq x batch for RNN\n",
    "\n",
    "        outputs, hidden = self.gru(output)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attention_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attention_energies = attention_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attention_energies[i] = hidden.dot(encoder_outputs[i])\n",
    "\n",
    "        # Squeeze to range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.sigmoid(attention_energies).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout=0.05):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Attention module\n",
    "        self.attention = Attention()\n",
    "    \n",
    "    def forward(self, context_input, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: Batching\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine context and embedded word, through RNN\n",
    "        rnn_input = torch.cat((context_input.unsqueeze(0), word_embedded), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attention_weights = self.attention(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn the whole thing into one module by combining the Encoder and Decoder networks. There's also an embedding layer for the context tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "class RARNN(nn.Module):\n",
    "    def __init__(self, input_size, output_tokens, hidden_size):\n",
    "        super(RARNN, self).__init__()\n",
    "        \n",
    "        self.output_tokens = output_tokens\n",
    "        self.input_size = input_size\n",
    "        self.output_size = len(output_tokens)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, hidden_size)\n",
    "        \n",
    "        self.encoder = Encoder(self.input_size, hidden_size)\n",
    "        self.decoder = Decoder(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, context_input, word_inputs, word_targets=None):\n",
    "        # Get embedding for context input\n",
    "        context_embedded = self.embedding(context_input)\n",
    "        \n",
    "        input_len = word_inputs.size(0)\n",
    "        target_len = word_targets.size(0) if word_targets is not None else MAX_LENGTH\n",
    "        \n",
    "        # Run through encoder\n",
    "        encoder_outputs, encoder_hidden = self.encoder(context_embedded, word_inputs)\n",
    "        decoder_hidden = encoder_hidden # Use encoder's last hidden state\n",
    "        decoder_input = Variable(torch.LongTensor([0])) # EOS/SOS token\n",
    "        if USE_CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "\n",
    "        # Variables to store decoder and attention outputs\n",
    "        decoder_outputs = Variable(torch.zeros(target_len, self.output_size))\n",
    "        decoder_attentions = Variable(torch.zeros(target_len, input_len))\n",
    "        if USE_CUDA:\n",
    "            decoder_outputs = decoder_outputs.cuda()\n",
    "            decoder_attentions = decoder_attentions.cuda()\n",
    "        \n",
    "        # Run through decoder\n",
    "        for i in range(target_len):\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(context_embedded, decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs[i] = decoder_output\n",
    "            decoder_attentions[i] = decoder_attention\n",
    "\n",
    "            # Teacher forcing with known targets, if provided\n",
    "            if word_targets is not None:\n",
    "                decoder_input = word_targets[i]\n",
    "\n",
    "            # Sample with last outputs\n",
    "            else:\n",
    "                max_index = decoder_output.topk(1)[1].data[0][0]\n",
    "                decoder_input = Variable(torch.LongTensor([max_index]))\n",
    "                if USE_CUDA:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "\n",
    "                if max_index == 0: break # EOS\n",
    "        \n",
    "        # Slice outputs\n",
    "        if word_targets is None:\n",
    "            decoder_outputs = decoder_outputs[:i]\n",
    "            decoder_attentions = decoder_attentions[:i]\n",
    "        elif target_len > 1:\n",
    "            decoder_attentions = decoder_attentions[:-1] # Ignore attentions on EOS\n",
    "\n",
    "        return decoder_outputs, decoder_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The inputs to the network are the current phrase label, e.g. `%getLightState` and the string to parse, \"the living room light is on\". The outputs are the child node labels, `$light` and `$on_off` with a selection of words given by attention-like weights over the sequence, treated as boolean values given a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "hidden_size = 100\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-6\n",
    "\n",
    "rarnn = RARNN(input_size, output_tokens, hidden_size)\n",
    "optimizer = torch.optim.Adam(rarnn.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "decoder_criterion = nn.NLLLoss()\n",
    "attention_criterion = nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(flat, node):\n",
    "    context, inputs, targets = data_for_node(flat, node)\n",
    "\n",
    "    # Turn inputs into tensors\n",
    "    context_var = tokens_to_tensor([context], rarnn.output_tokens, False)\n",
    "    context_var = Variable(context_var)\n",
    "    inputs_var = input_lang.tokens_to_tensor(inputs) # seq x batch x size\n",
    "    inputs_var = Variable(inputs_var)\n",
    "    target_tokens = [target_token for target_token, _ in targets]\n",
    "    target_ranges = [target_range for _, target_range in targets]\n",
    "    target_tokens_var = tokens_to_tensor(target_tokens, rarnn.output_tokens)\n",
    "    target_tokens_var = Variable(target_tokens_var)\n",
    "    target_ranges_var = ranges_to_tensor(target_ranges, len(inputs))\n",
    "    target_ranges_var = Variable(target_ranges_var)\n",
    " \n",
    "    # Run through model\n",
    "    decoder_outputs, attention_outputs = rarnn(context_var, inputs_var, target_tokens_var)\n",
    "\n",
    "    # Loss calculation and backprop\n",
    "    optimizer.zero_grad()\n",
    "    decoder_loss = decoder_criterion(decoder_outputs, target_tokens_var)\n",
    "    if len(targets) > 0:\n",
    "        attention_loss = attention_criterion(attention_outputs, target_ranges_var)\n",
    "    else:\n",
    "        attention_loss = 0\n",
    "    total_loss = decoder_loss + attention_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 5945d0d0f8e1c2083c22dd61 at 2017-06-17 18:01:03\n",
      "[log] 0m 8s (100) 4.2476\n",
      "[log] 0m 16s (200) 2.0866\n",
      "[log] 0m 26s (300) 4.7207\n",
      "[log] 0m 35s (400) 0.8425\n"
     ]
    }
   ],
   "source": [
    "import sconce\n",
    "job = sconce.Job('rarnn')\n",
    "job.plot_every = 20\n",
    "job.log_every = 100\n",
    "\n",
    "n_epochs = 5000\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    walked_flat, walked_tree = walk_tree(parsed, parsed['%'], None)\n",
    "    def _train(node): return train(walked_flat, node)\n",
    "    ds = descend(walked_tree, _train)\n",
    "    d = sum(ds) / len(ds)\n",
    "    job.record(i, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(context, inputs, node=None):\n",
    "    if node == None:\n",
    "        node = Node('parsed')\n",
    "        node.position = (0, len(inputs))\n",
    "    \n",
    "    # Turn data into tensors\n",
    "    context_var = tokens_to_tensor([context], rarnn.output_tokens, False)\n",
    "    context_var = Variable(context_var)\n",
    "    inputs_var = input_lang.tokens_to_tensor(inputs) # seq x batch x size\n",
    "    inputs_var = Variable(inputs_var)\n",
    "    \n",
    "    # Run through RARNN\n",
    "    decoder_outputs, attention_outputs = rarnn(context_var, inputs_var)\n",
    "    \n",
    "    # Given the decoder and attention outputs, gather contexts and inputs for sub-phrases\n",
    "    # Use attention values > 0.5 to select words for next input sequence\n",
    "\n",
    "    next_contexts = []\n",
    "    next_inputs = []\n",
    "    next_positions = []\n",
    "    \n",
    "    for i in range(len(decoder_outputs)):\n",
    "        max_value, max_index = decoder_outputs[i].topk(1)\n",
    "        max_index = max_index.data[0]\n",
    "        next_contexts.append(rarnn.output_tokens[max_index]) # Get decoder output token\n",
    "        a = attention_outputs[i]\n",
    "        next_input = []\n",
    "        next_position = []\n",
    "        for t in range(len(a)):\n",
    "            at = a[t].data[0]\n",
    "            if at > 0.5:\n",
    "                if len(next_position) == 0: # Start position\n",
    "                    next_position.append(t)\n",
    "                next_input.append(inputs[t])\n",
    "            else:\n",
    "                if len(next_position) == 1: # End position\n",
    "                    next_position.append(t - 1)\n",
    "        if len(next_position) == 1: # End position\n",
    "            next_position.append(t)\n",
    "        next_inputs.append(next_input)\n",
    "#         next_position = (next_position[0] + node.position[0], next_position[1] + node.position[0])\n",
    "        next_positions.append(next_position)\n",
    "\n",
    "    evaluated = list(zip(next_contexts, next_inputs, next_positions))\n",
    "\n",
    "    # Print decoded outputs\n",
    "    print('\\n(evaluate) %s %s -> %s' % (context, ' '.join(inputs), next_contexts))\n",
    "    \n",
    "    # Plot attention outputs\n",
    "    if SHOW_ATTENTION:\n",
    "        fig = plt.figure(figsize=(len(inputs) / 3, 99))\n",
    "        sub = fig.add_subplot(111)\n",
    "        sub.matshow(attention_outputs.data.squeeze(1).numpy(), vmin=0, vmax=1, cmap='hot')\n",
    "        plt.show(); plt.close()\n",
    "    \n",
    "    for context, inputs, position in evaluated:\n",
    "        print('-> context', context, 'inputs', inputs)\n",
    "        # Add a node for parsed sub-phrases and values\n",
    "        sub_node = Node(context)\n",
    "        sub_node.position = position\n",
    "        node.add(sub_node)\n",
    "        \n",
    "        # Recursively evaluate sub-phrases\n",
    "        if context[0] == '%':\n",
    "            if len(inputs) > 0:\n",
    "                evaluate(context, inputs, sub_node)\n",
    "            else:\n",
    "                print(\"WARNING: Empty inputs\")\n",
    "    \n",
    "        # Or add words directly to value node\n",
    "        elif context[0] == '$':\n",
    "            sub_node.add(' '.join(inputs))\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_and_print(context, inputs):\n",
    "    evaluated = evaluate(context, inputs)\n",
    "    print(' '.join(inputs))\n",
    "    print(evaluated)\n",
    "    return evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_and_print('%', \"hey maia what's the btc price\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_and_print('%', \"plz call me when the price of tesla is above 5 0\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_and_print('%', \"hey maia if the ethereum price is less than 2 0 then turn the living room light on\".split(' '))\n",
    "evaluate_and_print('%', \"hey maia what's the ethereum price\".split(' '))\n",
    "evaluate_and_print('%', \"hey maia play some Skrillex please and then turn the office light off\".split(' '))\n",
    "evaluate_and_print('%', \"turn the office light up and also could you please turn off the living room light and make the temperature of the bedroom to 6 thank you maia\".split(' '))\n",
    "evaluate_and_print('%', \"turn the living room light off and turn the bedroom light up and also turn the volume up\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse(s, cb):\n",
    "    words = tokenize_sentence(s)\n",
    "    print('words', words)\n",
    "    try:\n",
    "        evaluated = evaluate_and_print('%', words)\n",
    "        cb({'words': words, 'parsed': evaluated.to_json()})\n",
    "    except Exception:\n",
    "        cb({'error': \"Failed to evaluate\"})\n",
    "\n",
    "parse('heya shoot me an email if the price of bitcoin is bigger than 3 0 0 0 would ya', lambda r: print(\"response\", r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import somata\n",
    "service = somata.Service('maia:parser', {'parse': parse}, {'bind_port': 8855})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service.socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
